{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "fashion=tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test)=fashion.load_data()\n",
    "\n",
    "print (x_train.shape)\n",
    "print (y_train.shape)\n",
    "\n",
    "print (x_test.shape)\n",
    "print (y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   3   1   0   0   7   0  37   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   1   2   0  27  84\n",
      "   11   0   0   0   0   0   0 119   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  88 143\n",
      "  110   0   0   0   0  22  93 106   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   4   0  53 129 120\n",
      "  147 175 157 166 135 154 168 140   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   2   0  11 137 130 128\n",
      "  160 176 159 167 178 149 151 144   0   0]\n",
      " [  0   0   0   0   0   0   1   0   2   1   0   3   0   0 115 114 106 137\n",
      "  168 153 156 165 167 143 157 158  11   0]\n",
      " [  0   0   0   0   1   0   0   0   0   0   3   0   0  89 139  90  94 153\n",
      "  149 131 151 169 172 143 159 169  48   0]\n",
      " [  0   0   0   0   0   0   2   4   1   0   0   0  98 136 110 109 110 162\n",
      "  135 144 149 159 167 144 158 169 119   0]\n",
      " [  0   0   2   2   1   2   0   0   0   0  26 108 117  99 111 117 136 156\n",
      "  134 154 154 156 160 141 147 156 178   0]\n",
      " [  3   0   0   0   0   0   0  21  53  92 117 111 103 115 129 134 143 154\n",
      "  165 170 154 151 154 143 138 150 165  43]\n",
      " [  0   0  23  54  65  76  85 118 128 123 111 113 118 127 125 139 133 136\n",
      "  160 140 155 161 144 155 172 161 189  62]\n",
      " [  0  68  94  90 111 114 111 114 115 127 135 136 143 126 127 151 154 143\n",
      "  148 125 162 162 144 138 153 162 196  58]\n",
      " [ 70 169 129 104  98 100  94  97  98 102 108 106 119 120 129 149 156 167\n",
      "  190 190 196 198 198 187 197 189 184  36]\n",
      " [ 16 126 171 188 188 184 171 153 135 120 126 127 146 185 195 209 208 255\n",
      "  209 177 245 252 251 251 247 220 206  49]\n",
      " [  0   0   0  12  67 106 164 185 199 210 211 210 208 190 150  82   8   0\n",
      "    0   0 178 208 188 175 162 158 151  11]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=128\n",
    "EPOCH=50\n",
    "cls_num=len(set(y_train))\n",
    "image_shape=(28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(features,feature_columns,output_cls):\n",
    "    \n",
    "    input_layer=tf.feature_column.input_layer(features,feature_columns)\n",
    "    \n",
    "    net = tf.reshape(input_layer, [-1, 28, 28, 1]) \n",
    "    \n",
    "#     tf.layers.conv2d   isdeprecated ,keras.layers.conv2d\n",
    "    conv1=tf.keras.layers.Conv2D(filters=16,kernel_size=[3,3],activation='relu',name='conv1')(net)\n",
    "    pool1=tf.keras.layers.MaxPool2D(pool_size=(3,3))(conv1)\n",
    "    conv2=tf.keras.layers.Conv2D(filters=8,kernel_size=[5,5],activation='relu',name='conv2')(pool1)\n",
    "    flat=tf.keras.layers.Flatten()(conv2)\n",
    "    dens1=tf.keras.layers.Dense(units=128,activation='relu',name='dense1')(flat)\n",
    "    logits=tf.keras.layers.Dense(units=output_cls,name='dense_output')(dens1)\n",
    "    return logits\n",
    "def  model_fn_builder(lr):\n",
    "    # 该方法实际 创建 estimator的model_fn\n",
    "    # 可以 有其他操作\n",
    "    def model_fn(features, labels, mode, params,config): #estimator需要的model_fn 参数固定\n",
    "        '''\n",
    "        features: from input_fn的返回  切记返回的顺序\n",
    "        labels： from input_fn 的返回  切记返回的顺序\n",
    "        mode: tf.estimator.ModeKeys实例的一种\n",
    "        params: 在初始化estimator时 传入的参数列表，dict形式,或者直接使用self.params也可以\n",
    "        config:初始化estimator时 的 Runconfig\n",
    "\n",
    "        '''\n",
    "        logits=create_model(features,params['feature_columns'],params['output_cls'])\n",
    "        \n",
    "        pre_cls=tf.math.argmax(input=logits,axis=1)\n",
    "        pre_prob=tf.nn.softmax(logits=logits,axis=1)\n",
    "        \n",
    "        is_predict=mode==tf.estimator.ModeKeys.PREDICT\n",
    "        if not is_predict:\n",
    "            # train .eval\n",
    "            loss=tf.losses.sparse_softmax_cross_entropy(labels=tf.cast(labels,tf.int32),logits=logits)\n",
    "\n",
    "            def metric_fn(labels,predictions):\n",
    "                '''\n",
    "                define metrics\n",
    "                '''\n",
    "                accuracy,accuracy_update=tf.metrics.accuracy(labels=labels,predictions=predictions,name='image_accuracy')\n",
    "    #                 auc=tf.metrics.auc(labels=labels,predictions=predictions,name='iris_auc')\n",
    "                recall,recall_update=tf.metrics.recall(labels=labels,predictions=predictions,name='image_recall')\n",
    "                precision,precision_update=tf.metrics.precision(labels=labels,predictions=predictions,name='image_precision')\n",
    "    #                 with tf.control_dependencies([recall,precision]):\n",
    "    #                     f1_score=f1(recall=recall,precision=precision)\n",
    "\n",
    "                return {\n",
    "                    'accuracy':(accuracy,accuracy_update),\n",
    "                    'recall':(recall,recall_update),\n",
    "                    'precision':(precision,precision_update)                  \n",
    "                }\n",
    "\n",
    "\n",
    "            if mode==tf.estimator.ModeKeys.EVAL:\n",
    "                return tf.estimator.EstimatorSpec(mode=mode,loss=loss,eval_metric_ops=metric_fn(labels,pre_cls))\n",
    "\n",
    "            # train process\n",
    "            train_op=tf.train.AdamOptimizer(learning_rate=lr).minimize(loss=loss,global_step=tf.train.get_global_step())\n",
    "            return tf.estimator.EstimatorSpec(mode=mode,loss=loss,train_op=train_op,eval_metric_ops=metric_fn(labels,pre_cls))\n",
    "\n",
    "\n",
    "        else:\n",
    "            predictions={'predict_cls':pre_cls,'predict_pro':pre_prob}\n",
    "            return tf.estimator.EstimatorSpec(mode=mode,predictions=predictions)     \n",
    "    return model_fn\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_builder(x,y,batch_size,epochs,is_train=True):\n",
    "    '''\n",
    "    创建 输入函数闭包\n",
    "    '''\n",
    "    # 可以执行其它操作\n",
    "    \n",
    "    def input_fn():\n",
    "        dataset=tf.data.Dataset.from_tensor_slices(({'images':x},y) )  \n",
    "        if is_train:\n",
    "            dataset=dataset.shuffle(1000).repeat(epochs)\n",
    "        dataset=dataset.batch(batch_size)\n",
    "        return dataset # 返回的 顺序要和 model_fn一致 或者 dataset元素 格式为（features,label）元组 也可以\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir=r'F:\\testDemo\\AI\\estimator\\model\\fashion'\n",
    "params={}\n",
    "feature_columns=[tf.feature_column.numeric_column('images',shape=image_shape)]\n",
    "\n",
    "output_cls=cls_num\n",
    "params['feature_columns']=feature_columns\n",
    "params['output_cls']=output_cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NumericColumn(key='images', shape=(28, 28), default_value=None, dtype=tf.float32, normalizer_fn=None)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'F:\\\\testDemo\\\\AI\\\\estimator\\\\model\\\\fashion', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 100, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000002AC8409FE80>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# tf.summary.FileWriterCache.clear()  # 该步骤 只有当您 对 model_dir进行过删除 又重建（相同路径），导致没有events生产时，再添加\n",
    "config=tf.estimator.RunConfig(save_checkpoints_steps=100)\n",
    "\n",
    "estimator=tf.estimator.Estimator(model_fn=model_fn_builder(0.001),model_dir=model_dir,params=params,config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt-20000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 20000 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.14470702, step = 20001\n",
      "INFO:tensorflow:Saving checkpoints for 20100 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 21.2633\n",
      "INFO:tensorflow:loss = 0.20766848, step = 20101 (4.705 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 20200 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.3848\n",
      "INFO:tensorflow:loss = 0.19923146, step = 20201 (3.523 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 20300 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.3307\n",
      "INFO:tensorflow:loss = 0.09380077, step = 20301 (3.408 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 20400 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.2415\n",
      "INFO:tensorflow:loss = 0.13611808, step = 20401 (3.419 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 20500 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.2947\n",
      "INFO:tensorflow:loss = 0.15494746, step = 20501 (3.665 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 20600 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.8228\n",
      "INFO:tensorflow:loss = 0.13108248, step = 20601 (3.352 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 20700 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.2152\n",
      "INFO:tensorflow:loss = 0.1887804, step = 20701 (3.423 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 20800 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.1026\n",
      "INFO:tensorflow:loss = 0.15873241, step = 20801 (3.985 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 20900 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.1859\n",
      "INFO:tensorflow:loss = 0.19600993, step = 20901 (3.426 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 21000 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.33\n",
      "INFO:tensorflow:loss = 0.22708067, step = 21001 (3.529 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 21100 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 30.146\n",
      "INFO:tensorflow:loss = 0.11221834, step = 21101 (3.317 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 21200 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.0245\n",
      "INFO:tensorflow:loss = 0.1996505, step = 21201 (3.445 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 21300 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.2756\n",
      "INFO:tensorflow:loss = 0.23158477, step = 21301 (3.417 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 21400 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.0658\n",
      "INFO:tensorflow:loss = 0.09797695, step = 21401 (3.439 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 21500 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 30.3771\n",
      "INFO:tensorflow:loss = 0.14420637, step = 21501 (3.292 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 21600 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.923\n",
      "INFO:tensorflow:loss = 0.19508952, step = 21601 (3.342 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 21700 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.793\n",
      "INFO:tensorflow:loss = 0.18454912, step = 21701 (3.357 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 21800 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 30.21\n",
      "INFO:tensorflow:loss = 0.08158271, step = 21801 (3.309 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 21900 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.174\n",
      "INFO:tensorflow:loss = 0.19149017, step = 21901 (3.429 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 22000 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.2449\n",
      "INFO:tensorflow:loss = 0.12859567, step = 22001 (3.539 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 22100 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.6324\n",
      "INFO:tensorflow:loss = 0.096106336, step = 22101 (3.375 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 22200 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.1308\n",
      "INFO:tensorflow:loss = 0.16319656, step = 22201 (3.687 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 22300 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.4692\n",
      "INFO:tensorflow:loss = 0.27834427, step = 22301 (3.639 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 22400 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.1643\n",
      "INFO:tensorflow:loss = 0.09806293, step = 22401 (3.553 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 22500 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.3392\n",
      "INFO:tensorflow:loss = 0.107759334, step = 22501 (3.409 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 22600 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.9474\n",
      "INFO:tensorflow:loss = 0.09976564, step = 22601 (3.336 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 22700 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.7752\n",
      "INFO:tensorflow:loss = 0.13963969, step = 22701 (3.736 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 22800 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.1925\n",
      "INFO:tensorflow:loss = 0.114670694, step = 22801 (3.546 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 22900 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.2896\n",
      "INFO:tensorflow:loss = 0.12641723, step = 22901 (3.414 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 23000 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.6351\n",
      "INFO:tensorflow:loss = 0.11983202, step = 23001 (3.493 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 23100 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.4802\n",
      "INFO:tensorflow:loss = 0.187183, step = 23101 (3.511 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 23200 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.3713\n",
      "INFO:tensorflow:loss = 0.20457067, step = 23201 (3.653 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 23300 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.6894\n",
      "INFO:tensorflow:loss = 0.20444328, step = 23301 (3.369 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 23400 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.9464\n",
      "INFO:tensorflow:loss = 0.19654872, step = 23401 (3.454 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 23500 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 21.2952\n",
      "INFO:tensorflow:loss = 0.17535095, step = 23501 (4.697 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 23600 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.5077\n",
      "INFO:tensorflow:loss = 0.17186035, step = 23601 (3.920 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 23700 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.6106\n",
      "INFO:tensorflow:loss = 0.16009715, step = 23701 (3.757 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 23800 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.3757\n",
      "INFO:tensorflow:loss = 0.1167527, step = 23801 (3.791 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 23900 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 20.8501\n",
      "INFO:tensorflow:loss = 0.19209112, step = 23901 (4.798 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 24000 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.7945\n",
      "INFO:tensorflow:loss = 0.17386952, step = 24001 (3.875 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 24100 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.2126\n",
      "INFO:tensorflow:loss = 0.122896485, step = 24101 (3.815 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 24200 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 24.7605\n",
      "INFO:tensorflow:loss = 0.20588404, step = 24201 (4.039 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 24300 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 23.0314\n",
      "INFO:tensorflow:loss = 0.12184756, step = 24301 (4.342 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 24400 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.1141\n",
      "INFO:tensorflow:loss = 0.13233414, step = 24401 (3.688 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 24500 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.1202\n",
      "INFO:tensorflow:loss = 0.3072877, step = 24501 (3.828 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 24600 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.8592\n",
      "INFO:tensorflow:loss = 0.06882848, step = 24601 (3.724 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 24700 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.4107\n",
      "INFO:tensorflow:loss = 0.13286898, step = 24701 (3.647 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 24800 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.5355\n",
      "INFO:tensorflow:loss = 0.090466216, step = 24801 (3.633 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 24900 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 22.2641\n",
      "INFO:tensorflow:loss = 0.14691994, step = 24901 (4.495 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 25000 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.91\n",
      "INFO:tensorflow:loss = 0.14021629, step = 25001 (3.581 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 25100 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.9843\n",
      "INFO:tensorflow:loss = 0.1872268, step = 25101 (3.573 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 25200 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.3072\n",
      "INFO:tensorflow:loss = 0.13265607, step = 25201 (3.533 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 25300 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.8835\n",
      "INFO:tensorflow:loss = 0.117205374, step = 25301 (3.719 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 25400 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.2802\n",
      "INFO:tensorflow:loss = 0.21461724, step = 25401 (3.536 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 25500 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.6855\n",
      "INFO:tensorflow:loss = 0.1529781, step = 25501 (3.488 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 25600 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.909\n",
      "INFO:tensorflow:loss = 0.13059267, step = 25601 (3.581 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 25700 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.4182\n",
      "INFO:tensorflow:loss = 0.1065035, step = 25701 (3.521 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 25800 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.5397\n",
      "INFO:tensorflow:loss = 0.21660551, step = 25801 (3.502 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 25900 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.3438\n",
      "INFO:tensorflow:loss = 0.1620196, step = 25901 (3.528 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 26000 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.1858\n",
      "INFO:tensorflow:loss = 0.14818469, step = 26001 (3.547 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 26100 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.726\n",
      "INFO:tensorflow:loss = 0.09606427, step = 26101 (3.366 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 26200 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.6495\n",
      "INFO:tensorflow:loss = 0.04906171, step = 26201 (3.493 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 26300 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.4492\n",
      "INFO:tensorflow:loss = 0.086761005, step = 26301 (3.514 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 26400 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.4755\n",
      "INFO:tensorflow:loss = 0.09278059, step = 26401 (3.390 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 26500 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.3383\n",
      "INFO:tensorflow:loss = 0.20433222, step = 26501 (3.530 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 26600 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.5903\n",
      "INFO:tensorflow:loss = 0.17633648, step = 26601 (3.498 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 26700 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.9628\n",
      "INFO:tensorflow:loss = 0.15375182, step = 26701 (3.576 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 26800 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.1523\n",
      "INFO:tensorflow:loss = 0.13802063, step = 26801 (3.551 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 26900 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.7447\n",
      "INFO:tensorflow:loss = 0.17121504, step = 26901 (3.479 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 27000 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.2628\n",
      "INFO:tensorflow:loss = 0.16745311, step = 27001 (3.668 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 27100 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.8842\n",
      "INFO:tensorflow:loss = 0.1352432, step = 27101 (3.587 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 27200 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.592\n",
      "INFO:tensorflow:loss = 0.16623239, step = 27201 (3.496 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 27300 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.2504\n",
      "INFO:tensorflow:loss = 0.1097242, step = 27301 (3.419 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 27400 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 26.8894\n",
      "INFO:tensorflow:loss = 0.105163045, step = 27401 (3.722 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 27500 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.3474\n",
      "INFO:tensorflow:loss = 0.21584569, step = 27501 (3.525 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 27600 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.9936\n",
      "INFO:tensorflow:loss = 0.20436823, step = 27601 (3.572 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 27700 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.1437\n",
      "INFO:tensorflow:loss = 0.11910425, step = 27701 (3.684 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 27800 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 29.1959\n",
      "INFO:tensorflow:loss = 0.07668105, step = 27801 (3.426 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 27900 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 24.3857\n",
      "INFO:tensorflow:loss = 0.12739226, step = 27901 (4.100 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 28000 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 25.3982\n",
      "INFO:tensorflow:loss = 0.12389478, step = 28001 (3.938 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 28100 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 23.1332\n",
      "INFO:tensorflow:loss = 0.12654938, step = 28101 (4.324 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 28200 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.4123\n",
      "INFO:tensorflow:loss = 0.16295114, step = 28201 (3.646 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 28300 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.3025\n",
      "INFO:tensorflow:loss = 0.08963808, step = 28301 (3.665 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 28400 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.3233\n",
      "INFO:tensorflow:loss = 0.14725712, step = 28401 (3.947 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 28500 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 23.7083\n",
      "INFO:tensorflow:loss = 0.21262664, step = 28501 (4.217 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 28600 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.3153\n",
      "INFO:tensorflow:loss = 0.09441452, step = 28601 (3.661 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 28700 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.5418\n",
      "INFO:tensorflow:loss = 0.12913972, step = 28701 (3.632 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 28800 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 21.9696\n",
      "INFO:tensorflow:loss = 0.09775399, step = 28801 (4.551 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 28900 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.2162\n",
      "INFO:tensorflow:loss = 0.111220464, step = 28901 (3.544 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 29000 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 27.0544\n",
      "INFO:tensorflow:loss = 0.13343765, step = 29001 (3.696 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 29100 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.6942\n",
      "INFO:tensorflow:loss = 0.13000385, step = 29101 (3.484 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 29200 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 22.248\n",
      "INFO:tensorflow:loss = 0.14935227, step = 29201 (4.496 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 29300 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 18.0306\n",
      "INFO:tensorflow:loss = 0.22410892, step = 29301 (5.546 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 29400 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 24.5289\n",
      "INFO:tensorflow:loss = 0.10301001, step = 29401 (4.077 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 29500 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 25.0561\n",
      "INFO:tensorflow:loss = 0.079598576, step = 29501 (3.991 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 29600 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.6634\n",
      "INFO:tensorflow:loss = 0.16431484, step = 29601 (3.489 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 29700 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 24.4613\n",
      "INFO:tensorflow:loss = 0.1074519, step = 29701 (4.090 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 29800 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.8093\n",
      "INFO:tensorflow:loss = 0.13793263, step = 29801 (3.469 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 29900 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 28.0121\n",
      "INFO:tensorflow:loss = 0.096660286, step = 29901 (3.573 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 30000 into F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.23107104.\n"
     ]
    }
   ],
   "source": [
    "train=estimator.train(input_fn=input_fn_builder(x=x_train,y=y_train,batch_size=BATCH_SIZE,epochs=EPOCH,is_train=True),steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-06-06T06:50:05Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt-30000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-06-06-06:50:06\n",
      "INFO:tensorflow:Saving dict for global step 30000: accuracy = 0.8644, global_step = 30000, loss = 0.7665193, precision = 0.9817285, recall = 0.9731111\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 30000: F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt-30000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.8644,\n",
       " 'loss': 0.7665193,\n",
       " 'precision': 0.9817285,\n",
       " 'recall': 0.9731111,\n",
       " 'global_step': 30000}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.evaluate(input_fn=input_fn_builder(x=x_test,y=y_test,batch_size=BATCH_SIZE,epochs=EPOCH,is_train=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn_builder(x,batch_size):\n",
    "    '''\n",
    "    创建 输入函数闭包\n",
    "    '''\n",
    "    # 可以执行其它操作\n",
    "    \n",
    "    def input_fn():\n",
    "        dataset=tf.data.Dataset.from_tensor_slices(({'images':x}) )  \n",
    "\n",
    "        dataset=dataset.batch(batch_size)\n",
    "        return dataset # 返回的 顺序要和 model_fn一致 或者 dataset元素 格式为（features,label）元组 也可以\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 0, 3, 0, 2, 7, 2, 5, 5], dtype=uint8)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_dataset=x_train[0:10]\n",
    "pre_y=y_train[0:10]\n",
    "pre_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=estimator.predict(input_fn=predict_fn_builder(x=x_train[0:10],batch_size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt-30000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "{'predict_cls': 9, 'predict_pro': array([0.0000000e+00, 9.7136024e-32, 4.8922935e-37, 2.4414274e-34,\n",
      "       2.6000109e-38, 5.0287287e-18, 7.1402950e-31, 8.2528209e-07,\n",
      "       3.1106856e-26, 9.9999917e-01], dtype=float32)}\n",
      "{'predict_cls': 0, 'predict_pro': array([9.9999535e-01, 4.0889493e-34, 2.9028916e-15, 6.8556294e-09,\n",
      "       3.9609319e-15, 8.1493402e-24, 4.6315395e-06, 2.0719370e-12,\n",
      "       1.2751717e-27, 3.6395057e-27], dtype=float32)}\n",
      "{'predict_cls': 0, 'predict_pro': array([9.9918240e-01, 5.6884212e-14, 6.2230316e-07, 6.4789201e-05,\n",
      "       2.0265060e-10, 3.8250029e-14, 7.5217051e-04, 1.1419887e-18,\n",
      "       4.0469545e-19, 1.0197527e-30], dtype=float32)}\n",
      "{'predict_cls': 6, 'predict_pro': array([8.4250763e-02, 3.3461257e-15, 2.9126289e-13, 2.9308584e-01,\n",
      "       1.5119933e-11, 7.4126037e-17, 6.2266308e-01, 3.2409324e-07,\n",
      "       7.5717800e-20, 7.0771971e-19], dtype=float32)}\n",
      "{'predict_cls': 0, 'predict_pro': array([9.5720196e-01, 6.1046006e-03, 1.8754016e-11, 3.5721440e-02,\n",
      "       1.1038952e-11, 2.0867729e-19, 9.7194972e-04, 3.7313208e-17,\n",
      "       5.6631888e-17, 7.1978824e-24], dtype=float32)}\n",
      "{'predict_cls': 2, 'predict_pro': array([5.3014111e-07, 3.1882837e-29, 9.9999356e-01, 5.7459650e-23,\n",
      "       6.2342070e-10, 0.0000000e+00, 5.9204581e-06, 2.9051021e-30,\n",
      "       1.1086539e-23, 1.8631410e-27], dtype=float32)}\n",
      "{'predict_cls': 7, 'predict_pro': array([6.61900606e-24, 0.00000000e+00, 0.00000000e+00, 2.71915030e-32,\n",
      "       0.00000000e+00, 2.09421813e-25, 1.35108555e-20, 1.00000000e+00,\n",
      "       1.83153775e-30, 2.03190374e-22], dtype=float32)}\n",
      "{'predict_cls': 2, 'predict_pro': array([1.0009290e-14, 3.5013583e-32, 9.9801183e-01, 3.8271048e-27,\n",
      "       1.9662476e-03, 4.1366399e-32, 2.1944619e-05, 3.1791603e-23,\n",
      "       1.1301711e-15, 1.4305268e-19], dtype=float32)}\n",
      "{'predict_cls': 5, 'predict_pro': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)}\n",
      "{'predict_cls': 5, 'predict_pro': array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "for p in predictions:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " save as image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "def save_image(filename, data_array):\n",
    "    im = Image.fromarray(data_array.astype('uint8'))\n",
    "    im_invert = ImageOps.invert(im)\n",
    "    im_invert.save(filename)\n",
    "\n",
    "i=0\n",
    "for item in x_train:\n",
    "    i+=1\n",
    "    filename='{}/{}.jpg'.format('data/fashion/images',str(i))    \n",
    "    save_image(filename=filename,data_array=item)\n",
    "    if i==10:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Restoring parameters from F:\\testDemo\\AI\\estimator\\model\\fashion\\model.ckpt-30000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: export_base/fashion\\temp-b'1559803909'\\saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'export_base/fashion\\\\1559803909'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def serving_input_receiver_fn():\n",
    "    def decode_and_resize(image_str_tensor):\n",
    "        \"\"\"Decodes jpeg string, resizes it and returns a uint8 tensor.\"\"\"\n",
    "        image = tf.image.decode_jpeg(image_str_tensor, channels=1) # 在此处 是 使用灰度图，所以channel=1\n",
    "        image = tf.expand_dims(image, 0)\n",
    "        image = tf.image.resize_bilinear(\n",
    "             image, [28, 28], align_corners=False)# 根据您 自己图的 大小设置 height width\n",
    "        image = tf.squeeze(image, squeeze_dims=[0])\n",
    "        image = tf.cast(image, dtype=tf.uint8)\n",
    "        return image\n",
    "\n",
    "     # Optional; currently necessary for batch prediction.\n",
    "    key_input = tf.placeholder(tf.string, shape=[None]) \n",
    "    key_output = tf.identity(key_input)\n",
    "\n",
    "    input_ph = tf.placeholder(tf.string, shape=[None], name='image_binary')\n",
    "    images_tensor = tf.map_fn(\n",
    "      decode_and_resize, input_ph, back_prop=False, dtype=tf.uint8)\n",
    "    receiver_tensors = {'image_bytes': input_ph}\n",
    "   \n",
    "    features = {\n",
    "       'images': images_tensor\n",
    "    }    \n",
    "    return tf.estimator.export.ServingInputReceiver(features,receiver_tensors)\n",
    "\n",
    "estimator.export_savedmodel('export_base/fashion',serving_input_receiver_fn=serving_input_receiver_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 请求体\n",
    "{\"instances\" : [{\"image_bytes\": {\"b64\":\"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APf6yfEuvW/hnw/d6vcjdHbpkIDguScAD6k1heHfiVomuQKZJhbS7QW3/cz35H3focV0Nrr2l3sbSQXsTKrbTk45xnofYg/jWlmvHPjzrdk2g2OjxTwTXTXnnSRI+541RGGSB0yWHXHtnnHhayiFo3t1khkAHIYg59RzxX0L8FkvP+EGkmt54JhPeySO0xO8PtVWBx15XgntivNfGupeMB4o1K2vdVv4P3zmO3ErpF5efl2qDgjGORn35rkoFWCYm4tXuEZCH2Hy2Vuu5WIOCD6gggkHrkRi0juHdIYliV+WWWcuWx0zgBf0r2X4O319o/hO8t4dMmu0bUHfzIVLLnZGMZ/CvXNR0nTtXgEGpWFreRA5CXESyAH1AI4NZUPgPwjbvvj8M6SG9TaIf5irY8LeHl6aDpY+lnH/AIVqRxRwxrHEioi8BVGAPwr/2Q==\"}},\n",
    "{\"image_bytes\": {\"b64\":\"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AO9+LPjO88KaPZQabKsN9fyMFlKhikaAbiAeM5ZRznqa8eitvEWqLFqk3iuG2luot6NdazLDM0YlMY4UH5d+QBTRqXibweltf2HiSOeC4eQRtZ37XMLum3cHVhjd868kZ5619K6FqkeuaDp+qRLtS7t0m2/3SwBI/A8fhXN+Ovh3YeNfLuLi8vYLm3iZIfJdSnPPKsD3x0x0FfP9l4r8R6XZQWdlrFxbQQAiOONI8JnJOMqT1LdT3rd8LQXvxJ8U2+meItXvZ4I45ZwyeWrrwo4wuOcLnIPAr6G0HRbXw7oltpNk0rW9upCGVtzHJJOT9Sa0a+MXbnJJOMDmvQPgy23x/D/tWsyfop/pX0dRXy5/witnO+Tc3S7iSQpT1Pqtdr8NPDltpnjG1uIri5dhFLxIUxyuOyivbq//2Q==\"}}\n",
    "]}\n",
    "# 返回\n",
    "{\n",
    "    \"predictions\": [\n",
    "        {\n",
    "            \"predict_pro\": [\n",
    "                0.0011323,\n",
    "                0.000574304,\n",
    "                0.00000434548,\n",
    "                0.0000422241,\n",
    "                1.86852e-8,\n",
    "                3.08857e-13,\n",
    "                0.00000731479,\n",
    "                5.13255e-9,\n",
    "                0.99824,\n",
    "                1.81437e-10\n",
    "            ],\n",
    "            \"predict_cls\": 8\n",
    "        },\n",
    "        {\n",
    "            \"predict_pro\": [\n",
    "                7.66782e-8,\n",
    "                4.37143e-13,\n",
    "                0.000230125,\n",
    "                3.37866e-9,\n",
    "                0.998791,\n",
    "                7.81147e-10,\n",
    "                0.000125866,\n",
    "                1.52834e-8,\n",
    "                0.00085307,\n",
    "                1.49522e-16\n",
    "            ],\n",
    "            \"predict_cls\": 4\n",
    "        }\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_NER",
   "language": "python",
   "name": "ner"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
